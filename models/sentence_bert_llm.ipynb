{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuu34ooOBHj-"
   },
   "source": [
    "SentenceTransformers: https://sbert.net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tU5XusSLHNDI"
   },
   "source": [
    "# Load Data (Without Reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n420CGlSIbBp"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kM7LlpfuHUx0",
    "outputId": "57c387e1-e95e-4a74-c9fb-6b6970a254bf"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJFcHzKbHfEd",
    "outputId": "0d1b0dcb-1359-4f98-c0cb-714786918ac3"
   },
   "outputs": [],
   "source": [
    "file_path = '/content/drive/MyDrive/Colab Notebooks/totallymakescents/fra_standard.csv'\n",
    "with open(file_path, encoding='latin1') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        print(f\"Line {i+1}: {row}\")\n",
    "        if i >= 9:  # print first 10 lines\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hl8Dt45wNeX7"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=',',            # specify correct separator\n",
    "    encoding='latin1',\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UOO2mMUzKqWW",
    "outputId": "b6e630ca-9db7-4d3d-8753-23d301f241c4"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0K1hF3l2K71i",
    "outputId": "84225f0a-4dc9-455e-9d4b-cc5914febd98"
   },
   "outputs": [],
   "source": [
    "print(f\"Shape: {df.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 889
    },
    "id": "oDG6J_giLL1B",
    "outputId": "d3cb1ec0-298f-49a5-ac8b-a2bbb3c86376"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HThyHAjVZVzj"
   },
   "source": [
    "# Load Data (with Reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wLusj5qZiSF",
    "outputId": "5b295e74-704d-41f6-8e89-19e683835b74"
   },
   "outputs": [],
   "source": [
    "file_path = '/content/drive/MyDrive/Colab Notebooks/totallymakescents/perfumes_table.csv'\n",
    "with open(file_path, encoding='latin1') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for i, row in enumerate(reader):\n",
    "        print(f\"Line {i+1}: {row}\")\n",
    "        if i >= 9:  # print first 10 lines\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgxnDak4coz0"
   },
   "outputs": [],
   "source": [
    "df_review = pd.read_csv(\n",
    "    file_path,\n",
    "    sep=',',            # specify correct separator\n",
    "    encoding='latin1',\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yHqxcfk_ckFa",
    "outputId": "dccd6892-28b0-4ec2-c6d5-33a9fd754c12"
   },
   "outputs": [],
   "source": [
    "df_review.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2qPdpT5WdEi2",
    "outputId": "14e3bdaa-1b58-475f-fc43-c7603781c1e7"
   },
   "outputs": [],
   "source": [
    "print(f\"Shape: {df_review.shape}\")  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 869
    },
    "id": "V7511l_LdHy-",
    "outputId": "1ad41277-9775-4730-8dce-ed83c67f3bd3"
   },
   "outputs": [],
   "source": [
    "df_review.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2JKUBI3eb0a"
   },
   "source": [
    "# Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0vgtpOZfO_N"
   },
   "outputs": [],
   "source": [
    "df['url'] = df['url'].str.strip().str.lower()\n",
    "df_review['url'] = df_review['url'].str.strip().str.lower()\n",
    "\n",
    "# remove trailing \"/\"\n",
    "df['url'] = df['url'].str.rstrip('/')\n",
    "df_review['url'] = df_review['url'].str.rstrip('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dceBQSJ8ejBS"
   },
   "outputs": [],
   "source": [
    "# concatinate all non-nan reviews into a single string\n",
    "agg_reviews = df_review.groupby('url')['reviews'].apply(lambda x: ' '.join(x.dropna().astype(str))).reset_index()\n",
    "\n",
    "combined_df = pd.merge(df, agg_reviews, on='url', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "id": "7ypNvXTVeoke",
    "outputId": "161a936b-5b28-4d1d-9b73-639e7e366e54"
   },
   "outputs": [],
   "source": [
    "print(combined_df.columns)\n",
    "print(combined_df.shape)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oub2BnlLgnLq",
    "outputId": "4de94de8-89da-45a4-905d-0f1b86ec430d"
   },
   "outputs": [],
   "source": [
    "non_null_reviews = combined_df['reviews'].notnull().sum()\n",
    "print(f\"Perfumes with reviews: {non_null_reviews} / {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27TJ9GnkOReG"
   },
   "source": [
    "# Sentence-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KcJwy5gz_W_L",
    "outputId": "d7bed8f2-deb6-4979-b60b-8d5c3d1bf8ac"
   },
   "outputs": [],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "taiKIduwBU4F"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQksnAIRB5sG"
   },
   "outputs": [],
   "source": [
    "# load a pretrained Sentence Transformer model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Aoc7EW8Cltm"
   },
   "outputs": [],
   "source": [
    "# select note columns\n",
    "note_cols = ['Top', 'Middle', 'Base',\n",
    "             'mainaccord1', 'mainaccord2', 'mainaccord3',\n",
    "             'mainaccord4', 'mainaccord5']\n",
    "\n",
    "# fill NaNs and combine into a string\n",
    "# df['scent_description'] = df[note_cols].fillna('').agg(' '.join, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAdF1VaPqrst"
   },
   "outputs": [],
   "source": [
    "combined_df['reviews'] = combined_df['reviews'].str.replace(r'^\\[?\"|\"?\\]$', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mucCVENoLGW"
   },
   "outputs": [],
   "source": [
    "# def build_full_description(row):\n",
    "#     scent_text = ' '.join([str(row.get(col, '')) for col in note_cols])\n",
    "#     review_text = str(row.get('reviews', ''))\n",
    "#     return (scent_text + ' ' + review_text).strip()\n",
    "\n",
    "def build_full_description(row):\n",
    "    # so that BERT can understand different fields\n",
    "\n",
    "    top = row.get('Top', '')\n",
    "    middle = row.get('Middle', '')\n",
    "    base = row.get('Base', '')\n",
    "\n",
    "    accords = ', '.join([\n",
    "        str(row.get('mainaccord1', '')),\n",
    "        str(row.get('mainaccord2', '')),\n",
    "        str(row.get('mainaccord3', '')),\n",
    "        str(row.get('mainaccord4', '')),\n",
    "        str(row.get('mainaccord5', ''))\n",
    "    ])\n",
    "\n",
    "    reviews = row.get('reviews', '')\n",
    "\n",
    "    description = (\n",
    "        f\"Top Notes: {top}. \"\n",
    "        f\"Middle Notes: {middle}. \"\n",
    "        f\"Base Notes: {base}. \"\n",
    "        f\"Main Accords: {accords}. \"\n",
    "        f\"User Review: {reviews}\"\n",
    "    )\n",
    "\n",
    "    return description.strip()\n",
    "\n",
    "combined_df['full_description'] = combined_df.apply(build_full_description, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYI4ItCTortZ",
    "outputId": "1ccc4d2b-21d5-48c2-ef74-4dffc2da5677"
   },
   "outputs": [],
   "source": [
    "combined_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "ROHIQl8cQKAr",
    "outputId": "695c9aa0-f954-4706-cd8e-3c16da2c5a26"
   },
   "outputs": [],
   "source": [
    "# see the change\n",
    "combined_df[['Top', 'Middle', 'Base',\n",
    "    'mainaccord1', 'mainaccord2', 'mainaccord3',\n",
    "    'mainaccord4', 'mainaccord5', 'full_description']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q5-Jx6ZepNmr",
    "outputId": "450c13ad-5eb4-48ad-f4ce-f8ec0988a05a"
   },
   "outputs": [],
   "source": [
    "print(combined_df['full_description'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "9395b5afa5dd4941b0bc2bf69e7f1f7b",
      "df0d3e9089ff4ba6b6fb09ac1a38f366",
      "0d5851f6f8f4441fafb2f417ecf88a29",
      "357b836e0a5045e1b70997e336f51aaf",
      "ef6e34e5e5544c34bb987d1cef4f8ab2",
      "50b843434d27403694699908bdf34232",
      "8cbbbd64c2a9405dbcad92a065c069dd",
      "8afa0565d0854fbdaaf88bdbc42a84cc",
      "c9ac567b21b042cdb570cca1d7c947e2",
      "cb55f6b499f94c589a52940ba93d7ae1",
      "ed60c9a64df84b71959b17191a7c4cae"
     ]
    },
    "id": "mRPwd0PRQ8Rc",
    "outputId": "1d2db1a5-55ab-4cba-9eb2-2eeaf9d339ed"
   },
   "outputs": [],
   "source": [
    "# now we are ready to build embeddings\n",
    "\n",
    "scent_lists_with_review = combined_df['full_description'].tolist()\n",
    "scent_embeddings = model.encode(\n",
    "    scent_lists_with_review,\n",
    "    batch_size=32,\n",
    "    convert_to_tensor=True,\n",
    "    show_progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4G_JAsOTg1u"
   },
   "outputs": [],
   "source": [
    "def recommend_perfumes(user_query, top_k=5):\n",
    "    query_embedding = model.encode(user_query, convert_to_tensor=True)\n",
    "    similarities = util.cos_sim(query_embedding, scent_embeddings)[0]\n",
    "    # print(similarities)\n",
    "    top_results = torch.topk(similarities, k=top_k)\n",
    "\n",
    "    print(f\"\\nUser Query: {user_query}\\n\")\n",
    "    for score, idx in zip(top_results.values, top_results.indices):\n",
    "        idx = idx.item()        # convert Python tensor to int\n",
    "        perfume = df.iloc[idx]\n",
    "        print(f\"{perfume['Perfume']} by {perfume['Brand']} (Score: {score.item():.3f})\")\n",
    "        print(f\"Notes: {perfume['scent_description']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJ3VrAeST0lS",
    "outputId": "7f88b659-48d5-4bd5-e457-55ae379f4ecc"
   },
   "outputs": [],
   "source": [
    "recommend_perfumes(\"A magical scent in a mystical forest with herbs and secrets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCqBfH4nYIp1",
    "outputId": "56a351dd-eb75-47f4-93be-b9be9afee141"
   },
   "outputs": [],
   "source": [
    "recommend_perfumes(\"What’s it smell like in the rain, at the end of a hiking trail full of blossoms?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6WRibNZCYQpL",
    "outputId": "3562fd37-85bb-43c9-fecb-59d499f31e38"
   },
   "outputs": [],
   "source": [
    "recommend_perfumes(\"What fragrance would a wizard wear in a magical world?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "budQUwAPYZJu",
    "outputId": "de01ad9c-f014-44a4-cb20-57bfa92d71e1"
   },
   "outputs": [],
   "source": [
    "recommend_perfumes(\"Looking for a bittersweet scent for a farewell party.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_PS_bMfPP5c"
   },
   "source": [
    "# LLM Generates Explanation\n",
    "Follows this notebook:\n",
    "https://colab.research.google.com/drive/1T5-zKWM_5OD21QHwXHiV9ixTRR7k3iB9?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpkI0cYMPTmx"
   },
   "outputs": [],
   "source": [
    "# Chat-GPT generated data, will replace later with data from Hugging Face\n",
    "# to train LLM to generate meaningful explanation, need more data ~100-200, will add more\n",
    "in_out_data = [\n",
    "    {\n",
    "    \"input\": \"User wants a scent for a farewell party. Scent: citrus, violet, ambergris.\",\n",
    "    \"output\": \"This perfume evokes a bittersweet goodbye — bright citrus for the goodbye, softened by violet and grounded in ambergris like a fading memory.\"\n",
    "    },\n",
    "    {\n",
    "    \"input\": \"User seeks a scent for a first date. Scent: raspberry, rose, sandalwood.\",\n",
    "    \"output\": \"Flirty and romantic — sweet raspberry teases at the top, while rose adds elegance, and sandalwood brings warm confidence.\"\n",
    "    },\n",
    "    {\n",
    "    \"input\": \"User asks for a scent that feels like winter morning in a snowy cabin. Scent: pine, incense, vanilla.\",\n",
    "    \"output\": \"This scent blends cool pine with smoky incense and cozy vanilla — a quiet snow-covered morning by the fire.\"\n",
    "    },\n",
    "    {\n",
    "    \"input\": \"User wants something that smells like a library filled with old books. Scent: leather, cedarwood, patchouli.\",\n",
    "    \"output\": \"Evoking antique leather and aged wood, this scent conjures dusty pages, quiet corners, and timeless thought.\"\n",
    "    },\n",
    "    {\n",
    "      \"input\": \"User needs a scent for celebrating a big achievement. Scent: champagne accord, white florals, musk.\",\n",
    "      \"output\": \"A sparkling celebration — fizzy champagne and bright florals rise with joy, grounded by elegant musk.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNU4G7TRYUJs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"perfume_reasoning_dataset.jsonl\", \"w\") as f:\n",
    "    for example in in_out_data:\n",
    "        json_line = json.dumps(example)\n",
    "        f.write(json_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zl0QFOGcWIk3"
   },
   "source": [
    "## Install Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vDyg_m0Ubpit"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345,
     "referenced_widgets": [
      "d1077568bc91413892cabf44c611654a",
      "63439219d75348bd957ba1494c2257c4",
      "ee2e287b9fad4692a59dd7abb7f537f5",
      "11cfb32e56604b2fa942fa95adb01c01",
      "60cf5ed2126a45c69071f70ceb9d6a1c",
      "bcc852af813d497ca3a38744aa32351e",
      "7e5e43ef973d49eeb340b0090de6e2e9",
      "b31c8b234b4e40399ec3acb93dbfae83",
      "b1968394b84b4710b9c07cd5d3acba00",
      "728d510ac62e41c3b4a30cbd2e61cdca",
      "b9d3fb13bec6456782a0fa2a2d16a90f",
      "3f352a270df74c5ca83dad17c734e93c",
      "e846ac174e11406b91b0902c03339cdc",
      "fe7404054ca94bf7a4fa5c6252ae2041",
      "52fe002b9d594a8893caeed9ea172538",
      "102dc89b6d904f4db5d2729e1c54f034",
      "9973a8ebf6344770a6c0736322ac6ceb",
      "282e815e9db54e1190389e7b30fa8793",
      "b07d5cc822fb4de1bc3c0ff63fae3b08",
      "2c02defdaa6f4dd2828cdb8938942d6e",
      "90deb31b719b4fcdbfafee547cc6be53",
      "feb543ce45bb4824b51ba2d090b65ca6",
      "a667b037211b42a38264ddb48ecc507f",
      "0b8b12886308447bb3842aa33a4da34e",
      "15ca0a4478e14929b602748843c3620f",
      "cbf90570936348e09e48b5b1e17df84f",
      "352d32f95fc347c2bb028c23f90f21a7",
      "49eb5f72f9cb46c59d36c575e55f479f",
      "e65cde6bdb7c449283833379b1bc5901",
      "d08ba8cdec4b4f508f836ca20ef3e6c4",
      "7fa6057b8e4848cba0488f034f807df3",
      "07d8cd0711a2412a8bf8278454d0c9ed",
      "beb09107c15840dcb7782c378efa7056",
      "ba8537a0732742efb76d95ae063c0038",
      "ea7599a61dbb4e02a52fe4cb1ed01a20",
      "f10158cd180b47d592f8a23bcc71b8cb",
      "42ff0f4a7ffc40eda4654721e955b867",
      "84ed0643bc644199950d6e38f865fa66",
      "320fd3ca1caf4948a893d7659dff03e0",
      "962afb1b242944298d2824aeb95e00c1",
      "b8aeb5d1e0034fb388c1a429d2fc3939",
      "dce3e06536fa48c4b85b87e8d2ec1fd0",
      "4a8025e476e64a7d85f64d1438bb5186",
      "4c944fc452a04d31a45189ac1d503289",
      "3f63ff082779469ca6e8c016b91dd056",
      "cc53aa87d47f41c28dc59af3b4080935",
      "46cece59ca364472bddc15071cc6d594",
      "85deced19f2748e6b6942d48db2346ae",
      "fd53078ffc814aba906ed197c1f0a7cb",
      "246942d17f4142a8b827c4b7ed16f206",
      "a8043fcbce0d466aa61c08fef3f7ff43",
      "f832defb7a4840e2bd54adc8e86ee330",
      "275ed7ad9c6d438abebb69b31a210226",
      "9f845319fd854768a3cb41b2578d37c8",
      "f4d4829607114c73904a0d87b7787a5f",
      "26ac11d030fb4f1db422781ca82ec6e6",
      "5d85a714f094438f99de0a05b460aff5",
      "7b29012349e440c599ed5b1f45075f90",
      "0dd1e2eef8b34e7a8a16395734203c17",
      "62811f5b758a4224b357256b4ddc23f1",
      "80758e7e102a4b1e9cd7e9815c3ae460",
      "41202d37d5fd4dabafe904a1ce21c616",
      "5670bd1d1bec460da5d39a041c5202fa",
      "e8d4247e5b0b4348be617d68b447e047",
      "afe890b5bb99444c95d565f2e7d538de",
      "2f921f3a1769498a9ec94616b639d695"
     ]
    },
    "id": "5VkF_t2ETePu",
    "outputId": "4f358f8d-7719-4784-cb60-ec28e588a4a3"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
    "fourbit_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 2x faster\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
    "    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # 4bit for 405b!\n",
    "    \"unsloth/Mistral-Small-Instruct-2409\",     # Mistral 22b 2x faster!\n",
    "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
    "    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n",
    "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
    "    \"unsloth/gemma-2-9b-bnb-4bit\",\n",
    "    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n",
    "\n",
    "    \"unsloth/Llama-3.2-1B-bnb-4bit\",           # NEW! Llama 3.2 models\n",
    "    \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-bnb-4bit\",\n",
    "    \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\", # or choose \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CsMqTNtfYhyS",
    "outputId": "15e4dc5f-e508-44f7-ed89-2967e1914f9c"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSD7gR6VZOXl"
   },
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7d4f40046f20472ca5e0026647c780df",
      "d1bf3697d0574b33a72a61ce59a67cfd",
      "e1aba94a2a494d47b6d3242533a0ab95",
      "30f7c4e2bda547c1b5c06ccd9c35d29a",
      "28e03b8da8a549d3b328501a4933857a",
      "f9fe906b32414150a0c9453661bfc2a5",
      "915ee15626204ac5bb494188bee2ff88",
      "07b652c835a34401babc1b88d1faec2c",
      "98c7a5ff3e544da390d44704dd14e456",
      "5203390cded140a2b6fe4fbd5f6b4029",
      "4a02a3caa2874440bcc6d2d7abd3d97b"
     ]
    },
    "id": "WXKciEi4fXIJ",
    "outputId": "76acecd2-2aea-48ee-a169-b3323b6afc6f"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"perfume_reasoning_dataset.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ff4dbfa716a746c4a909116d3133859a",
      "162475b7109e4ee897fbb7cca51f2155",
      "9ca7bcc4a7d846dc9eda0a927540e72c",
      "5d96f163ae474e3fb45e9612121de19f",
      "18926560f74442f49a99b2a9f0cf2855",
      "7d068932c09e4664a2fee4f6523b60c4",
      "141df5f1424347aebd85feee7f082972",
      "9103276f89a0488fac314b23c93ed373",
      "8beab1c5af0c43879bb47ad90b8c7ad0",
      "2e8effd0a456400c9c6b47ede2569e85",
      "36e5f9b4b4c04c81864f08dfb215d00d"
     ]
    },
    "id": "8JGLd1UpY07h",
    "outputId": "d9b9e6b7-5d57-4f41-dae7-521fd45a2ea7"
   },
   "outputs": [],
   "source": [
    "# needs this format when fine-tuning LLaMA3\n",
    "def formatting_prompts_func(example):\n",
    "    prompt = f\"<|start_header_id|>user<|end_header_id|>\\n\\n{example['input']}<|eot_id|>\\n\" \\\n",
    "             f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{example['output']}<|eot_id|>\"\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCytN4pLa_H8",
    "outputId": "0081a777-5c40-4ea0-db9d-2970d32c7bb8"
   },
   "outputs": [],
   "source": [
    "print(dataset[4][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lV0SedyLcTf2"
   },
   "source": [
    "## Finetune Model with LLama3.2-Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4738521eff9b4393bd15b491a97242f3",
      "4e6f656a84bb471cb432af3c2c81693a",
      "7ddad1ab150741aa9d4f67c186494719",
      "1553d5dabb64420fa4abe89c460745fd",
      "b9c1e0bca3b845ab90b1d93ff6b383c5",
      "91884b11523c419e896f08d6fdb93c61",
      "3b13fcba1de54339924803ae2229daeb",
      "a68490cb9b694557b633d8423dde7321",
      "5b2b473226574935946c711cfec786bb",
      "a36c49c1f8754fbdad6bb01ebac8f20b",
      "f5bd1d55c3004437a55899c5aa7c4da1"
     ]
    },
    "id": "Ln8sBiAFMo1n",
    "outputId": "4be634c3-5255-4d30-9385-2f04e44561e3"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "fe90b0f76f634cd89d644fd9a17b5827",
      "b429356a6be849d188273d7d32c16e00",
      "7b7cd059350d46f78ef09a6a8e314896",
      "2e6de7c5d26745d29245e4a9e4b6a79b",
      "11a253e0e17b4eb0a2a8b63fcf53e01a",
      "df4efb76b6e64503b9a8e51398fb15a3",
      "76f8c695bfe2435d81c040085e3f324d",
      "0838cb743c924b94bda12bda2b062528",
      "21a5c33845e44224a27ed2f08bdf4122",
      "7c0d3e06ef164cffa05b399d3819669e",
      "56d3d84ea07743b4a0dcd2ff44439e1d"
     ]
    },
    "id": "W6JiyDxub-2O",
    "outputId": "c3355625-0ebd-426e-aa23-550780d294e9"
   },
   "outputs": [],
   "source": [
    "# Only training on assistant responses\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "PS1l6hibcd3e",
    "outputId": "45ae9998-dbe3-4219-a912-9d932b36b8d7"
   },
   "outputs": [],
   "source": [
    "# Verifying data format\n",
    "\n",
    "tokenizer.decode(trainer.train_dataset[4][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Zd0dc1RBcjN8",
    "outputId": "9503fff6-8bea-4630-bba2-b5cdcab9acad"
   },
   "outputs": [],
   "source": [
    "space = tokenizer(\" \", add_special_tokens = False).input_ids[0]\n",
    "tokenizer.decode([space if x == -100 else x for x in trainer.train_dataset[4][\"labels\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "opmpmhWHdPbe",
    "outputId": "4f9280d7-5212-48ee-f97b-5f24da4fa87e"
   },
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ps5vbogodSIh",
    "outputId": "227fbd0b-3f5f-457a-f3dd-9137462135fd"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_tFe4Z8Ievr9",
    "outputId": "cb7957fb-a8e0-4798-a04b-ed6b4da43e1e"
   },
   "outputs": [],
   "source": [
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Hh9CJjoe7hc"
   },
   "source": [
    "## Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UiLRv_fwe1Gc",
    "outputId": "f5a06fa4-b13a-480e-e7d8-a38d0c4fcfcb"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"User wants a scent for a farewell party. Scent: citrus, violet, ambergris.\",\n",
    "    },\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(input_ids = inputs, max_new_tokens = 128, use_cache = True,\n",
    "                         temperature = 1.5, min_p = 0.1)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wYbU-Nz7fx04",
    "outputId": "ee5a172f-d9db-4fa6-f934-7ca292045b93"
   },
   "outputs": [],
   "source": [
    "# Use a TextStreamer for continuous inference\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"User wants a scent for a graduation ceremony. Scent: citrus, violet, ambergris.\",\n",
    "    },\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvqdY48FgJYi"
   },
   "outputs": [],
   "source": [
    "# overfitting because only has 5 data set in the in_out_data, needs to add more data\n",
    "# see a way for users to input the next prompt\n",
    "# use the output from sentence-bert as an input for LLM"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
